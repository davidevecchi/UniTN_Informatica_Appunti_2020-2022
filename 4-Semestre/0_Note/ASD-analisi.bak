# ASD ‚Äì Analisi di algoritmi

|      |           |                                                              |
| ---- | :-------: | -----------------------------------------------------------: |
| ASD  | Analisi di algoritmi | [üóÄ][root] [üóç](http://cricca.disi.unitn.it/montresor/teaching/asd/materiale/lucidi/) [üñ≠](http://cricca.disi.unitn.it/montresor/teaching/asd/materiale/video/) |

[TOC]





# [1.][pdf-1] Introduzione

## ==Definizioni==

> **Problema computazionale**: *relazione matematica che associa un elemento del dominio di output ad ogni elemento del dominio di input*

- Dati un **dominio** di input e uno di output

> **Algoritmo**: *procedimento effettivo, espresso tramite un insieme di passi elementari ben specificati in un sistema formale di calcolo, che risolve il problema in tempo finito*

- Dato un **problema** computazionale





## ~~Descrivere un algoritmo~~

~~La descrizione deve essere~~

- ~~Il pi√π possibile **formale**~~
- ~~**Indipendente** dal linguaggio (pseudo-codice)~~
- ~~**Dettagliata**~~



### ~~Pseudo-codice~~

[~~Vedi slide 9-10~~][pdf-1]





## Valutare un algoritmo

### ~~Introduzione~~

- **Efficienza**
  - Stabilire come valutare se un programma √® efficiente
  - Alcuni problemi non possono essere risolti in modo efficiente
  - Esistono soluzioni ottime: non √® possibile essere pi√π efficienti
- **Correttezza**
  - Dimostrazione matematica, descrizione informale
  - Alcuni problemi non possono essere risolti
  - Alcuni problemi vengono risolti in modo approssimato



### Efficienza

> **Complessit√†**: *analisi delle risorse impiegate da un algoritmo per risolvere un problema, in funzione della dimensione e dalla tipologia dell‚Äôinput*

> <u>Definizione</u>  (**Risorse**)
>
> - **Tempo**: tempo impiegato per completare l‚Äôalgoritmo
> - **Spazio**: quantit√† di memoria utilizzata
> - **Banda**: quantit√† di bit spediti (algoritmi distribuiti)



#### Definizioni di tempo

- **Wall-clock time**: tempo effettivamente impiegato per eseguire un algoritmo
- **Numero di operazioni rilevanti**: che caratterizzano lo scopo dell‚Äôalgoritmo
- **Numero di operazioni elementari**: eseguibili in tempo costante dalla CPU



### ==Correttezza==

> **Invariante**: *condizione sempre vera in un certo punto del programma*

> **Invariante di ciclo**: *condizione sempre vera all‚Äôinizio dell‚Äôiterazione di ciclo*

> **Invariante di classe**: *condizione sempre vera al termine dell‚Äôesecuzione di un metodo della classe*

[Esempi slide 25-26][pdf-1]



#### ==Algoritmi iterativi==

Il concetto di **invariante di ciclo** aiuta a dimostrare la correttezza di un **algoritmo iterativo**

- ==**Inizializzazione** (caso base)==
  
  ==La condizione √® vera alla prima iterazione di un ciclo==
  
- ==**Conservazione** (passo induttivo)==
  
  ==Se la condizione √® vera prima di un‚Äôiterazione del ciclo, allora rimane vera al termine (prima della successiva iterazione)==
  
- ==**Conclusione**==
  
  ==Al termine del ciclo, l‚Äôinvariante rappresenta la correttezza dell‚Äôalgoritmo==



### ==Complessit√†==

$$
\text{Complessit√†}\; :\; \text{Dimensione dell‚Äôinput}\ ‚Üí\ \text{Tempo}
$$

<p></p>

#### ==Dimensione dell‚Äôinput==

La taglia dell‚Äôinput pu√≤ essere il ==**numero** di==

- ==**Bit** necessari per rappresentarlo (**criterio di costo logaritmico**)==
- ==**Elementi** di cui √® costituito (**criterio di costo uniforme**)==

In molti casi

- Si assume che gli **elementi** sono rappresentati da **numero costante di bit**
- Le due misure **coincidono** a meno di una **costante moltiplicativa**

<p></p>

#### ==Tempo==

==Definito come **numero di istruzioni elementari**==

- ==Eseguibili in **tempo costante**==







# [2.][pdf-2] Analisi di algoritmi

~~<u>Obiettivo</u>~~

- ~~Stimare la complessit√† in tempo degli algoritmi~~

~~<u>Motivazioni</u>~~

- ~~Stimare il tempo impiegato per un dato input~~
- ~~Stimare il pi√π grande input gestibile in tempi ragionevoli~~
- ~~Confrontare l‚Äôefficienza di algoritmi diversi~~
- ~~Ottimizzare le parti pi√π importanti~~





## Modelli di calcolo

### Definizioni

> **Modello di calcolo**: *rappresentazione astratta di un calcolatore*

- ~~**Astrazione**: permette di nascondere i dettagli~~
- ~~**Realismo**: riflette la situazione reale~~
- ~~**Potenza matematica**: permette di trarre conclusioni formali sul costo~~



#### ~~Macchina di Turing~~

> ~~<u>Definizione</u>  (**Macchina di Turing**)~~
>
> ~~Macchina ideale che manipola i dati contenuti su un nastro di lunghezza infinita secondo un insieme prefissato di regole~~
>
> ~~Ad ogni passo~~
>
> - ~~Legge il simbolo sotto la testina~~
> - ~~Modifica il proprio stato interno~~
> - ~~Scrive un nuovo simbolo nella cella~~
> - ~~Muove la testina a destra o a sinistra~~

- ~~Fondamentale nello studio della **calcolabilit√†**~~
- ~~Di **basso livello**~~

<img src="algoritmi.assets/gnome-shell-screenshot-MY5MT0.png" alt="gnome-shell-screenshot-MY5MT0" style="zoom:32%;" />



#### Random Access Machine

> <u>Definizione</u>  (**Random Access Machine**)
>
> - **Memoria**
>   - Quantit√† infinita di celle di dimensione finita
>   - Accesso in tempo costante
>     - Indipendente dalla posizione
> - **Processore**
>   - Uno solo
>   - Set di istruzioni elementari simile a quelle reali
>     - Operazioni aritmetiche e logiche
>     - Istruzioni di controllo
> - **Costo delle istruzioni elementari**
>   - Uniforme
>   - Ininfluente ai fini della valutazione



### ==Funzione di complessit√†==

==L'**analisi** di un algoritmo permette di ricavare la sua **funzione di complessit√† $T(n)$**==

==La funzione **dipende** da==

- ==Taglia dell'**input $n$**==
- ==Esecuzione delle **istruzioni**==

==Per ognuna delle $m$ **istruzioni** sono specificati==

- ==**Costo $c_i$**: tempo costante richiesto per l'esecuzione==
  - ==Potenzialmente diverso per ogni istruzione==
- ==**#volte $k_i$**: numero di volte che viene eseguita==
  - ==Dipende dalla taglia dell'input $n$==



#### ==Tipi di algoritmi==

==La **complessit√†** si calcola diversamente per==

- ==**Algoritmi iterativi**: sommatoria dei costi moltiplicati per i rispettivi #volte==
  $$
  \colorbox{#ffe480}{$T(n)=\sum_{i=1}^{\rm \#istr}\ k_ic_i$}
  $$

-  ==**Algoritmi ricorsivi**: espansione dell'[equazione di ricorrenza](#Ricorrenze)==
  $$
  \colorbox{#ffe480}{$T(n)=
  \begin{cases}
  c & n\le n_0 \\
  f(T(g(n))) & n>n_0
  \end{cases}$}
  $$



### Esempi di analisi

[Vedi slide 9-12][pdf-2]



### Classi di complessit√†

$$
\begin{array}{c}
& f(n) && \rm Tipo & \\ \hline
& \log n && \rm logaritmico \\
& \sqrt n && \rm sublineare \\
& n && \rm lineare \\
& n\log n && \rm loglineare \\
& n^2 && \rm quadratico \\
& n^3 && \rm cubico \\
& 2^n && \rm esponenziale
\end{array}
$$





## ==Notazione asintotica==

### ==Definizioni==

> <u>Definizione</u>  (**Notazione $O\,$**)
>
> Sia $g(n)$ una funzione di costo
>
> Si indica con $O(g(n))$ l'insieme delle funzioni $f(n)$ tali per cui
> $$
> \exist\, c > 0,\ \exist\,m\ge 0\ : \ f(n)\le cg(n),\ \ \forall\, n\ge m
> $$

- $f(n)=O(g(n))$  ~~($f (n)\ \text{ √® ‚ÄúO grande‚Äù di }\ g(n)$)~~
- ==$g(n)$ √® un **limite asintotico superiore** di $f(n)$==
- $f(n)$ cresce al pi√π come $g(n)$

> <u>Definizione</u>  (**Notazione $Œ©\,$**)
>
> Sia $g(n)$ una funzione di costo
>
> Si indica con $Œ©(g(n))$ l'insieme delle funzioni $f(n)$ tali per cui
> $$
> \exist\, c > 0,\ \exist\,m\ge 0\ : \ f(n)\ge cg(n),\ \ \forall\, n\ge m
> $$

- $f(n)=Œ©(g(n))$  ~~($f (n)\ \text{ √® ‚ÄúOmega grande‚Äù di }\ g(n)$)~~
- ==$g(n)$ √® un **limite asintotico inferiore** di $f(n)$==
- $f(n)$ cresce almeno come $g(n)$

> <u>Definizione</u>  (**Notazione $Œò\,$**)
>
> Sia $g(n)$ una funzione di costo
>
> Si indica con $Œò(g(n))$ l'insieme delle funzioni $f(n)$ tali per cui
> $$
> \exist\, c_1, c_2 > 0,\ \exist\,m\ge 0\ : \ c_1g(n)\le f(n)\le c_1g(n),\ \ \forall\, n\ge m
> $$

- $f(n)=Œò(g(n))$  ~~($f (n)\ \text{ √® ‚ÄúTheta‚Äù di }\ g(n)$)~~
- ==$f(n)=Œò(g(n))\iff f(n)=O(g(n))\ \and\ f(n)=Œ©(g(n))$==
- ==$f(n)$ cresce **esattamente** come $g(n)$==



### Esercizi

[Vedi slide 18-27][pdf-2]





## Complessit√† di problemi e algoritmi

~~<u>Obiettivo</u>: riflettere su complessit√† di problemi e algoritmi~~

- ~~Ottimizzazioni~~
- ~~Efficienza massima~~
- ~~Rapporto fra problema computazionale e algoritmo~~



### Algoritmi aritmetici

[Esempi (slide 29-44)][pdf-2]



### Algoritmi e problemi

Complessit√† in **tempo** di un

- **Algoritmo**: quantit√† di tempo richiesta per input di dimensione $n$
  
  Per tutti gli input, l'algoritmo
  
  - $O(f (n))$: costa **al pi√π** $f (n)$
  - $Œ©(f (n))$: costa **almeno** $f (n)$
  - $Œò(f (n))$: richiede **esattamente** $Œò(f (n))$
  
- **Problema**: complessit√† in tempo relative a tutte le possibili soluzioni
  
  - $O(f (n))$: complessit√† del **miglior algoritmo** che risolve il problema
  - $Œ©(f (n))$: **tempo minimo dimostrabile** che un algoritmo pu√≤ impiegare
  - $Œò(f (n))$: **algoritmo ottimo**



#### Limiti alla complessit√† di un problema

> <u>Definizione</u>  (**Limiti superiore e inferiore di un problema**)
>
> Un problema ha complessit√† $\lang O,Œ©\rang (f (n))$ se esiste almeno un algoritmo avente complessit√† $\lang O,Œ©\rang (f (n))$ che lo risolve





## Tipologia dell‚Äôinput

~~<u>Obiettivo</u>: valutare gli algoritmi in base all‚Äôinput~~

- ~~In alcuni casi, gli algoritmi si comportano diversamente a seconda delle caratteristiche dell‚Äôinput~~
- ~~Conoscere in anticipo tali caratteristiche permette di scegliere il miglior algoritmo in quella situazione~~
- ~~Il problema dell‚Äôordinamento √® caratterizzato da questi concetti~~



### Tipologia di analisi

- **Caso pessimo**
  - La pi√π importante
  - Il tempo di esecuzione √® un **limite superiore** al tempo per qualsiasi input
  - ~~Per alcuni algoritmi, il caso peggiore si verifica molto spesso~~
- **Caso medio**
  - Difficile da definire in alcuni casi
  - **Distribuzione uniforme**
- **Caso ottimo**
  - Utile se si hanno **informazioni particolari** sull‚Äôinput



### Algoritmi di ordinamento

> <u>Problema</u>  (**Ordinamento**)
>
> - <u>Input</u>: sequenza $A = a_1, . . . , a_n$ di $n$ valori
> - <u>Output</u>: sequenza $B = b_1, . . . , b_n$ che sia una permutazione di $A$ tale per cui
>
> $$
> b_1 ‚â§\,. . . ‚â§ b_n
> $$

[Esempi (slide 49-64)][pdf-2]



#### Selection sort

> <u>Algoritmo</u>  (**Selection sort**)
>
> Cerca il minimo e lo mette in posizione corretta, riducendo il problema agli $n ‚àí 1$ restanti valori

- Approccio naif

```c++
void selectionSort(Item[] A, int n)
	for i = 1 to i = n-1 do
		int min = min(A, i, n)
		A[i] ‚Üî A[min]
```

```c++
int min(Item[] A, int i, int n) 
	int min = i
	for j = i+1 to n do
		if A[j] < A[min] then
            min = j
	return min
```

- **Complessit√† totale** (caso medio, pessimo e ottimo)

$$
\sum_{i=1}^{n-1}(n-1)\ =\ \sum_{i=1}^{n-1}i\ =\ \frac{n(n-1)}{2}\ =\ n^2-\frac n2\ =\ O(n^2)
$$



#### Insertion sort

> <u>Algoritmo</u>  (**Insertion sort**)
>
> Inserisce ogni elemento nella posizione corretta del sottovettore precedentemente ordinato, shiftando gli elementi gi√† presenti

- Simile all'ordinamento di un mazzo di carte

```c++
void insertionSort(Item[] A, int n)
    for i = 2 to n do
    	Item tmp = A[i]
        int j = i
        while j > 1 and A[j-1] > tmp do
            A[j] = A[j-1]
            j = j-1
        A[j] = tmp
```

- Efficiente per ordinare **piccoli insiemi** di elementi
- Il **costo dipende** da
  - **Dimensione** dell'input
  - **Distribuzione** dei dati dell'input



#### Merge sort

> <u>Algoritmo</u>  (**Merge sort**)
>
> Basato sulla tecnica divide-et-impera
>
> - <u>Divide</u>: divide il vettore di $n$ elementi in due sottovettori di $n/2$ elementi
> - <u>Impera</u>: chiama ricorsivamente se stesso sui due sottovettori
> - <u>Combina</u>: unisce e ordina i due sottovettori ordinati tramite merge
>
> <u>Idea</u>: due sottovettori ordinati si ordinano pi√π velocemente in unico vettore

```c++
void mergeSort(Item[] A, int first, int last)
    if first < last
        int mid = floor((first + last) / 2)
        mergeSort(A, first, mid)
        mergeSort(A, mid+1, last)
        merge(A, first, last, mid)
```

```c++
void merge(Item[] A, int first, int last, int mid)
    int i, j, k, h
    i = first
    j = mid+1
    k = first 
    
    while i <= mid and j <= last do
        if A[i] <= A[j] then
            B[k] = A[i]
            i = i+1
        else
            B[k] = A[j]
            j = j+1
		k = k+1
    
	j = last
    for h = mid downto i do
        A[j] = A[h]
        j = j-1
    for j = first to k-1 do
        A[j] = B[j]
```

- **Complessit√† di `merge`**

$$
O(n)
$$

- **Costo computazionale di `mergeSort`** (si assuma $n=2^k$)

$$
T(n)=
\begin{cases}
c & n=1 \\
2\,T(n/2)+dn & n>1
\end{cases}\\[40px]
O\Bigg(\sum_{i=0}^k 2^i¬∑\frac{n}{2^i}\Bigg)\ =\ O\Bigg(\sum_{i=0}^k n\Bigg)\ =\ O(k¬∑n)\ \iff\ O(n\log n)
$$







# [3.][pdf-3] Funzioni di costo

## ==Notazione asintotica==

### ==Espressioni polinomiali==

> <u>Definizione</u>  (**Espressione polinomiale**)
> $$
> f(n) = \sum_{i=0}^k a_in^i,\ \ a_k\ne0 \ \ \Rarr\ \ f(n)=Œò(n^k)
> $$

- **Limite superiore**

$$
\exist\,c>0,\ \exist\,m\ge0\ :\ f(n)\le cn^k,\ \ \forall\,n\ge m
\\[16px]
c\,\ge\, a_k+\sum_{i=0}^{k-1}|a_i|\,>\,0\,,\ \ \ m=1
$$

- **Limite inferiore**

$$
\exist\,d>0,\ \exist\,m\ge0\ :\ f(n)\ge dn^k,\ \ \forall\,n\ge m
\\[16px]
d\,\le\, a_k-\sum_{i=0}^{k-1}\frac{|a_i|}{n}\,>\,0\ \iff\ n\,>\,\frac{\sum_{i=0}^{k-1}|a_i|}{a_k}
$$



#### ==Casi particolari==

==Funzioni **limitate** superiormente e inferiormente==
$$
\exist\,c_1,c_2>0,\ \exist\,m\ge0\ :\ c_1\le f(n)\le c_2,\ \forall\, n\ge m\ \ \Rarr\ \ \colorbox{#ffe480}{$f(n)=Œò(1)$}
$$



### ==Propriet√† delle notazioni==

- ==**Dualit√†**==
$$
\colorbox{#ffe480}{$f (n) = O(g(n)) \iff g(n) = Œ©(f (n))$}
$$

- ==**Eliminazione delle costanti**==
$$
\colorbox{#ffe480}{$f (n) = „ÄàO,Œ©„Äâ(g(n)) \iff af (n) =„ÄàO,Œ©„Äâ(g(n))\,,\ \ ‚àÄa > 0$}
$$

- ==**Sommatoria** (sequenza di algoritmi)==
$$
\colorbox{#ffe480}{$f_1 (n) =„ÄàO,Œ©„Äâ(g_1 (n))\,,\, \ f_2 (n) =„ÄàO,Œ©„Äâ(g_2 (n))\ \ \Rarr \\ \Rarr \ \ f_1 (n) + f_2 (n) =„ÄàO,Œ©„Äâ(\max(g_1 (n),\, g_2 (n)))$}
$$

- ==**Prodotto** (cicli annidati)==
$$
\colorbox{#ffe480}{$f_1 (n) =„ÄàO,Œ©„Äâ(g_1 (n))\,,\, \ f_2 (n) =„ÄàO,Œ©„Äâ(g_2 (n))\ \ \Rarr \\  \Rarr \ \ f_1 (n) ¬∑ f_2 (n) =„ÄàO,Œ©„Äâ(g_1 (n) ¬∑ g_2 (n))$}
$$

- ==**Simmetria**==
$$
\colorbox{#ffe480}{$f (n) = Œò(g(n)) \iff g(n) = Œò(f (n))$}
$$

- ==**Transitivit√†**==
$$
\colorbox{#ffe480}{$f (n) = O(g(n))\,,\,\ g(n) = O(h(n))\,\ \Rarr\,\ f (n) = O(h(n))$}
$$

[Dimostrazioni (slide 7-12)][pdf-3]



### ==Propriet√† delle funzioni==

==**Logaritmi**==

- ==$\log_a n = Œò(\log n)$==
- ==$\log n^a = Œò(\log n)\,,\ \forall\,a>0$==

==**Esponenziali**==

- ==$2^n=O((2+a)^n)\,,\ \forall\,a\ge0$==
- ==$2^{n+a}=Œò(2^n )\,,\, \, \ \ \ \ \ \  \forall\,a\ge0$==



### Notazioni $o$, $œâ$

> <u>Definizione</u>  (**Notazione $o$**)
>
> Sia $g(n)$ una funzione di costo
>
> Si indica con $o(g(n))$ l‚Äôinsieme delle funzioni $f (n)$ tali per cui
> $$
> ‚àÄ\,c,\ ‚àÉ\,m\ :\ f (n) < cg(n),\ ‚àÄ\,n ‚â• m
> $$

> <u>Definizione</u>  (**Notazione $œâ$**)
>
> Sia $g(n)$ una funzione di costo
>
> Si indica con $œâ(g(n))$ l‚Äôinsieme delle funzioni $f (n)$ tali per cui
> $$
> ‚àÄ\,c,\ ‚àÉ\,m\ :\ f (n) > cg(n),\ ‚àÄ\,n ‚â• m.
> $$

- $f (n) = „Äào,œâ„Äâ(g(n))$  ~~($f (n) \text{ √® „Äà‚Äúo piccolo‚Äù, ‚Äúomega piccolo‚Äù„Äâ di } g(n)$)~~

$$
\begin{array}{l}f(n) =„Äào,œâ„Äâ(g(n))\ \Rarr\ f (n) =„ÄàO,Œ©„Äâ(g(n))\end{array}
\\[16px]
\lim_{n‚Üí‚àû}\frac{f(n)}{g(n)}\ =\
\begin{cases}
\begin{array}{l}
0 &\Rarr\ \ f(n) = o(g(n))
\\
c \ne 0 &\Rarr\ \ f(n) = Œò(g(n))
\\
+‚àû &\Rarr\ \ f(n) = œâ(g(n))
\end{array}
\end{cases}
$$



### ==Classificazione delle funzioni==

==**Ordinamento** delle principali espressioni==
$$
\colorbox{#ffe480}{$\forall\ \ r < s,\ h < k,\ a < b$}
\\
\colorbox{#ffe480}{$\begin{array}{l}
 & O(1) &‚äÇ& O(\log^r n) &‚äÇ& O(\log^s n) &‚äÇ \\
‚äÇ& O(n^h) &‚äÇ& O(n^h \log^r n) &‚äÇ& O(n^h \log^s n) &‚äÇ \\
‚äÇ& O(n^k) &‚äÇ& O(a^n) &‚äÇ& O(b^n)
\end{array}$}
$$





## Ricorrenze

~~(~~[~~Torna a Tipi di algoritmi~~](#*Tipi di algoritmi*)~~)~~

### Definizioni

> **Equazione di ricorrenza**: *esprime la complessit√† di un algoritmo ricorsivo*

- Formula matematica definita in maniera ricorsiva
- Utilizzata per risolvere i problemi

> **Formula chiusa**: *rappresenta la classe di complessit√† della funzione*

- Obiettivo dell'analisi (quando possibile)



### Analisi per livelli

> <u>Metodo</u>  (**Albero di ricorsione / Per livelli**)
>
> Espandere l'equazione di ricorrenza in un albero, i cui nodi rappresentano i costi ai vari livelli della ricorsione

[Esempi (slide 22-25)][pdf-3]



### Metodo della sostituzione

> <u>Metodo</u>  (**Sostituzione / Tentativi**)
>
> Dimostrare per induzione la correttezza di una soluzione, dedotta sulla base della propria esperienza

$$
T(n)=
\begin{cases}
c & n\le n_0 \\
f_1(T(f_2(n))) & n>n_0
\end{cases}
$$

1. **Dedurre** una possibile soluzione
   $$
   T(n) = Œò(\boldsymbol g(n))
   $$

2. Formulare una **ipotesi induttiva**
   $$
   \boldsymbol h(n)=Œò(g(n))\ \ \Rarr\ \ \forall\,k<n,\ \ T(k)\ \lang \le,\ge\rang\ \boldsymbol {d\,h}(k)
   $$

3. **Sostituire** nella ricorrenza le espressioni $T (f_2(n))$ con l'ipotesi induttiva
   $$
   T(n)=\lang O,Œ©\rang(f_2(n))\ \iff\ T(n)\ \lang \le,\ge\rang\ f_1(\boldsymbol {d\,h}(f_2(n)))
   $$

4. Dimostrare che la soluzione √® valida anche per il **caso base**

[Esempi e tecniche (slide 27-37)][pdf-3]



### Metodo dell‚Äôesperto

> **Ricorrenze comuni**: *ricorrenze facilmente risolubili ricorrendo a teoremi specifici per ogni classe particolare di equazioni di ricorrenza*

[Vedi slide 39-55][pdf-3]



### Analisi degli algoritmi

[Esempi (slide 56-65)][pdf-3]







# [4.][pdf-4] Analisi ammortizzata

## Introduzione

> <u>Definizione</u>  (**Analisi ammortizzata**)
>
> Tecnica di analisi di complessit√† che valuta il **tempo richiesto** per eseguire, nel caso pessimo, una sequenza di operazioni su una **struttura dati**

- Esistono operazioni pi√π o meno costose
- Se le operazioni pi√π costose sono poco frequenti, allora il loro costo pu√≤ essere ammortizzato dalle operazioni meno costose

<u>Differenze</u>

- **Analisi caso medio**:      probabilistica, su singola operazione
- **Analisi ammortizzata**:  deterministica, su operazioni multiple, caso pessimo



### Metodi per l‚Äôanalisi

> <u>Metodo</u>  (**Aggregazione**)
>
> Complessit√† $T(n)$ per eseguire $n$ operazioni in sequenza nel caso pessimo
>
> - **Sequenza**: evoluzione della struttura data una sequenza di operazioni
> - **Caso pessimo**: peggior sequenza di operazioni
> - **Aggregazione**: sommatoria di tutte le complessit√† individuali

- ~~Tecnica derivata dalla matematica~~
  - ~~*Si calcola il costo complessivo (aggregato) e si divide per $n$*~~

<p></p>

> <u>Metodo</u>  (**Ammortamento / Accantonamenti**)
>
> Alle operazioni vengono assegnati **costi ammortizzati** che possono essere maggiori/minori del loro costo effettivo
>
> - Si assegna un costo ammortizzato potenzialmente distinto ad ognuna delle operazioni possibili
>
> - Il costo ammortizzato pu√≤ essere diverso dal costo effettivo
>
>   - Operazioni meno costose caricate di un costo aggiuntivo (**credito**)
>
>   $$
>   \text{costo ammortizzato = costo effettivo + credito prodotto}
>   $$
>
> - I crediti accumulati sono usati per pagare le operazioni pi√π costose
>   $$
>   \text{costo ammortizzato = costo effettivo ‚àí credito consumato}
>   $$

- ~~Tecnica derivata dall‚Äôeconomia~~

<u>Obiettivi</u>: dimostrare che

- La somma dei costi ammortizzati $a_i$ √® un limite superiore ai costi effettivi $c_i$
  $$
  \sum_{i=1}^n c_i\ \le\ \sum_{i=1}^n a_i
  $$

- Il valore cos√¨ ottenuto √® poco costoso

<u>Note</u>

- La dimostrazione deve valere per tutte le sequenze (caso pessimo)

- Il credito dopo l'operazione $t$-esima √® sempre positivo
  $$
  \sum_{i=1}^t a_i -\sum_{i=1}^t c_i\ \ge\ 0
  $$

<p></p>

> <u>Metodo</u>  (**Potenziale**)
>
> Lo stato del sistema viene descritto tramite **differenze di potenziale**
>
> - Si associa alla struttura dati $D$ una **funzione di potenziale** $Œ¶(D)$
>
>   - Le operazioni meno costose incrementano $Œ¶(D)$
>   - Le operazioni pi√π costose decrementano $Œ¶(D)$
>
> - Costo ammortizzato
>   $$
>   \text{costo ammortizzato = costo effettivo + variazione di potenziale}
>   $$
>
>   $$
>   a_i = c_i + Œ¶(D_i) - Œ¶(D_{i-1})
>   $$
>
>   $D_i$ √® il potenziale associato alla $i$-esima operazione
>
> - Costo ammortizzato, sequenza di $n$ operazioni
>   $$
>   \begin{align}
>   A\ =\ \sum_{i=1}^n a_i  
>   \ =\ \sum_{i=1}^n(c_i + Œ¶(D_i) - Œ¶(D_{i‚àí1})) 
>   \ =\ C+Œ¶(D_n) - Œ¶(D_0)
>   \end{align}
>   $$
>   Se la variazione di potenziale $Œ¶(D_n ) ‚àí Œ¶(D_0 )$ √® non negativa, il costo ammortizzato $A$ √® un limite superiore al costo reale

- ~~Tecnica derivata dalla fisica~~





## Contatore binario

> <u>Esempio</u>  (**Contatore binario**)
>
> Contatore binario di $k$ bit con un vettore $A$ di booleani $0/1$
>
> - Bit meno significativo in $A[0]$, bit pi√π significativo in $A[k ‚àí 1]$
>
> - Valore del contatore
>   $$
>   x=\sum_{i=0}^{k-1}A[i]¬∑2^i
>   $$
>
> - Operazione $\tt increment()$ che incrementa il contatore di $1$
>
>   ```c++
>   void increment(bool[] A, int k)
>       int i = 0
>       while i < k and A[i] == 1 do
>           A[i] = 0
>           i = i+1
>       if i < k then
>   		A[i] = 1
>   ```



### Aggregazione

<u>Analisi grossolana</u>

- Una chiamata $\tt increment()$ richiede tempo $O(k)$ nel caso pessimo
- Essendoci una sola operazione, esiste un‚Äôunica sequenza possibile
- Limite superiore $T (n) = O(nk)$ per una sequenza di n incrementi

<u>Costo delle operazioni</u>

- $n=2^k$ operazioni nel caso pessimo
- Costo di $n$ operazioni:  $\,T(n)=O(nk)=O(n\log n)$
- Costo di $1$ operazione: $\,T(n)/n=O(k)=O(\log n)$

<u>Analisi pi√π stretta</u>

- Il tempo necessario ad eseguire l‚Äôintera sequenza √® proporzionale al numero di bit che vengono modificati
- $A[i]$ viene modificato ogni $2^i$ incrementi

<u>Analisi ammortizzata</u>

- Costo totale
  $$
  T(n)\ =\ \sum_{i=0}^{k-1}\bigg\lfloor\frac{n}{2^i}\bigg\rfloor\ \le\ n\sum_{i=0}^{k-1}\frac{1}{2^i}\ \le\ n\sum_{i=0}^{+\infin}\bigg(\frac{1}{2}\bigg)^i\ =\ 2n\ =\ O(n)
  $$

- Costo ammortizzato
  $$
  T (n)/n\ =\ 2n/n\ =\ 2\ =\ O(1)
  $$



### Ammortamento

<u>Costo delle operazioni</u>

- **Costo effettivo** dell‚Äôoperazione $\tt increment()$: $\,d$

  $d$ √® il numero di bit che cambiano valore

- **Costo ammortizzato** dell‚Äôoperazione $\tt increment()$: $\,2$

  - $1$ per cambio di un bit da $0$ a $1$ (costo effettivo)
  - $1$ per il futuro cambio dello stesso bit da $1$ a $0$  ~~(dimostrazione)~~

<u>Conseguenze</u>

- In ogni istante, il credito √® pari al numero di bit $1$ presenti
- **Costo totale**: $\,O(n)$
- **Costo ammortizzato**: $\,O(1)$

<u>Dimostrazione</u>
$$
T (n)\ =\ \sum_{i=1}^n c_i\ \le\ \sum_{i=1}^n 2\ =\ 2n\ =\ O(n)
$$

$$
T (n)/n\ =\ 2n/n\ =\ 2\ =\ O(1)
$$



### Potenziale

Sia $Œ¶(D)$ il numero bit $1$ presenti nel contatore

- Operazione $\tt increment()$

  - **Costo effettivo**:                 $\,1+ t$  ~~($1$ bit $0‚Üí1$ e $t$ bit $1‚Üí0$)~~
  - **Variazione di potenziale**: $\,1 ‚àí t$  ~~($t$ bit sono diventati $0$)~~
  - **Costo ammortizzato**:        $\,(1+ t) +(1 ‚àí t) =2$

  $t$ √® il numero di bit $1$ dal meno significativo al primo bit $0$

- **All‚Äôinizio**:  zero bit accesi $\,\Rarr\ Œ¶(D_0) = 0$

- **Alla fine**:  $\,Œ¶(D_n ) ‚â• 0\ \ \ \ \ \,\Rarr$  la differenza di potenziale √® non negativa





## Vettori dinamici

### Inserimento

Per implementare una **sequenza** tramite vettori dinamici

- Si alloca un vettore di una certa dimensione $m$ detta **capacit√†**

L‚Äô**inserimento** di un elemento

- In mezzo ha costo $O(n)$ ~~(shift)~~
- In fondo ha costo $O(1)$ (append)

<u>Problema</u>

- Non √® noto a priori il numero di elementi da inserire nella sequenza
- La capacit√† selezionata pu√≤ rivelarsi insufficiente

<u>Soluzione</u>

1. Si alloca un vettore di capacit√† maggiore
   - Incremento fisso  (`java.util.Vector`)
   - Raddoppiamento (`java.util.ArrayList`)
2. Si copia il contenuto del vecchio vettore nel nuovo e si rilascia il vecchio

<u>Domanda</u>

- Qual √® la migliore politica di incremento?



#### Incremento

<u>Assunzioni</u>

- Incremento: $\,d$
- Dimensione iniziale: $\,d$
- Costo di scrittura di un elemento: $\,1$

<u>Analisi ammortizzata</u>

- **Costo effettivo** di un'operazione $\tt add()$
  $$
  c_i=
  \begin{cases}
  i & (i\ {\rm mod}\ d)=1 \\
  1 & \rm altrimenti
  \end{cases}
  $$

- **Costo effettivo** di $n$ operazioni $\tt add()$
  $$
  \begin{align}
  T(n)\ &=\ \sum_{i=1}^n c_i\ =\ n+\sum_{j=1}^{\lfloor n/d\rfloor} d¬∑j\ =\ n + d\ \frac{(\lfloor n/d\rfloor+1)\lfloor n/d\rfloor}{2} \\
  & \le\ \ n + \frac{(n/d+1)\, n}{2}\ =\ O(n^2)
  \end{align}
  $$

- **Costo ammortizzato** di un'operazione $\tt add()$
  $$
  T (n)/n\ =\ \frac{O(n^2)}n\ =\ O(n)
  $$



#### Raddoppiamento

<u>Assunzioni</u>

- Dimensione iniziale: $\,1$
- Costo di scrittura di un elemento: $\,1$

<u>Analisi ammortizzata</u>

- **Costo effettivo** di un'operazione $\tt add()$
  $$
  c_i=
  \begin{cases}
  i & i =2^k +1 \\
  1 & \rm altrimenti
  \end{cases}
  $$

- **Costo effettivo** di $n$ operazioni $\tt add()$
  $$
  \begin{align}
  T(n)\ &=\ \sum_{i=1}^n c_i\ =\ n+\sum_{j=0}^{\lfloor\log n\rfloor} 2^j\ =\ n + 2^{\lfloor\log n\rfloor+1} ‚àí 1 \\
  & \le\ n + 2^{\log n+1} ‚àí 1\ =\ n + 2n ‚àí 1\ =\ O(n)
  \end{align}
  $$

- **Costo ammortizzato** di un'operazione $\tt add()$
  $$
  T (n)/n\ =\ \frac{O(n)}n\ =\ O(1)
  $$



### Reality check

[Vedi slide 27-31][pdf-4]



### Cancellazione

La **rimozione** di un elemento ha costo

- $O(n)$ se il vettore √® ordinato
- $O(1)$ se il vettore non √® ordinato

<u>Problema</u>

- La cancellazione porta a sprechi di memoria quando $n\ll m$
  - $n$: numero di elementi attualmente presenti
  - $m$: capacit√†

<u>Soluzione</u>

- **Contrazione** quando il fattore di carico $Œ±=n/m$ diventa troppo piccolo
  - Allocazione, copia, deallocazione

<u>Domanda</u>

- Quale soglia per il fattore di carico?



#### Strategie - naif e ottimizzata

**Dimezzare** la memoria quando il fattore di carico $Œ±$ diventa $\frac12$ (naif)

<u>Problema</u>

- Numero di inserimenti/rimozioni insufficiente per ripagare le espansioni/contrazioni

<u>Soluzione</u>

- Lasciar decrescere il sistema ad un fattore inferiore a $Œ±=\frac14$
  - Dopo un‚Äôespansione, il fattore di carico diventa $\frac12$
  - Dopo una contrazione, il fattore di carico diventa $\frac12$

<u>Analisi ammortizzata</u>

- Si utilizzi una **funzione di potenziale** che

  - Vale $0$ all'inizio e subito dopo una espansione o contrazione
  - Cresce fino a $n$ quando $Œ±$ aumenta fino ad $1$ o diminuisce fino ad $\frac14$

  $$
  Œ¶=
  \begin{cases}
  2\,n ‚àí m & Œ± ‚â• \frac12 \\
  m/2 ‚àí n & Œ± ‚â§\frac12
  \end{cases}
  $$

  - Casi esplicativi

    - $Œ±=\frac12\ \text{(dopo espansione/contrazione)}\  \ \ \ \ \ \ \, \, \Rarr\ Œ¶ = 0$
    - $Œ± = 1\ \ \text{(prima di espansione)}\, \ \ \Rarr\ n = m\ \ \ \Rarr\ Œ¶ = n$
    - $Œ± =\frac14\ \text{(prima di contrazione)}\ \Rarr\ m = 4\,n\ \Rarr\ Œ¶ = n$

    Immediatamente prima di espansioni/contrazioni, il potenziale √® sufficiente per pagare il loro costo

- **Costo ammortizzato**: $\,a_i\ =\ c_i + Œ¶_i ‚àí Œ¶_{i‚àí1}\ =\ 3$   ([vedi slide 37][pdf-4])

![gnome-shell-screenshot-IIBEU0](algoritmi.assets/gnome-shell-screenshot-IIBEU0.png)





<p></p>

---

<p></p>



# Appendice

> **Padding**: *estensione dell'input*
>
> [Vedi slide 41][pdf-3]

<p></p>



# üóç

[root]: ../Algoritmi/1-Analisi
[pdf-0]: ../Algoritmi/1-Analisi/0-introcorso.pdf
[pdf-1]: ../Algoritmi/1-Analisi/1-introduzione.pdf
[pdf-1X]: ../Algoritmi/1-Analisi/1X-introduzione.pdf
[pdf-2]: ../Algoritmi/1-Analisi/2-analisi.pdf
[pdf-2X]: ../Algoritmi/1-Analisi/2X-analisi.pdf
[pdf-3]: ../Algoritmi/1-Analisi/3-funzioni.pdf
[pdf-3X]: ../Algoritmi/1-Analisi/3X-funzioni.pdf
[pdf-4]: ../Algoritmi/1-Analisi/4-analisi-ammortizzata.pdf